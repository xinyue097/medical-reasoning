# MedReason Configuration
model_name_or_path: "Qwen/Qwen3-8B-Base"
model_revision: main
torch_dtype: bfloat16
attn_implementation: sdpa
trust_remote_code: true

# LoRA Configuration
use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Dataset Configuration  
dataset_name: "openlifescienceai/medmcqa"
dataset_prompt_column: "question"
dataset_train_split: "train" 
dataset_test_split: "validation"
max_steps: -1

# Training Configuration
output_dir: "../../outputs/qwen_3_base_8B"
overwrite_output_dir: false
# resume_from_checkpoint: "./outputs/medreason/checkpoint-200"  # Specify exact checkpoint
seed: 42
bf16: true
use_vllm: true
vllm_mode: colocate
learning_rate: 1.0e-06
num_train_epochs: 3
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
# gradient_checkpointing: false
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
warmup_ratio: 0.1

# Torch Compile - DISABLED due to cuda memory issues
torch_compile: false
# torch_compile_mode: "reduce-overhead"

# Batch Configuration
per_device_train_batch_size: 16
per_device_eval_batch_size: 24
gradient_accumulation_steps: 2
generation_batch_size: 128
dataloader_num_workers: 0
dataloader_pin_memory: false 

# Sequence Lengths
max_prompt_length: 256
max_completion_length: 2048

# Evaluation
do_eval: true
eval_strategy: "steps"
eval_steps: 1000
eval_delay: 100

# GRPO Parameters
num_generations: 4
temperature: 0.6
top_p: 0.95


# Reward Functions
reward_funcs:
  - "multiple_choice"

reward_weights:
  - 1.0

# Full reward set:
# reward_funcs:
#   - "multiple_choice"
#   - "tag_count"
#   - "reasoning_steps"
#   - "repetition_penalty"
#   - "format"
#   - "format_partial_credit"
# reward_weights:
#   - 1.0
#   - 0.5
#   - 0.5
#   - 0.4
#   - 0.3
#   - 0.5

# Logging - Enhanced for better monitoring
log_level: "warning"
logging_strategy: "steps"
logging_steps: 5
logging_first_step: true
log_completions: true
report_to: ["wandb"]
save_strategy: "steps"
save_steps: 500
save_total_limit: 2

# Optimizer
optim: "adamw_torch"
weight_decay: 0.01
use_liger_kernel: true

# System prompt
system_prompt: |
  You are a medical expert providing reasoned responses. You must think through your reasoning process completely before answering.

  MANDATORY FORMAT: <think>[detailed reasoning]</think><answer>The answer is: [letter]</answer>

  REASONING REQUIREMENTS: In your thinking, analyze the medical question by breaking down key components, recalling relevant medical knowledge, evaluating each answer choice with supporting evidence, comparing viable options while considering clinical context and guidelines, then confirming your final selection. Your reasoning must be thorough enough for another expert to follow your logic completely.