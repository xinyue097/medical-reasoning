# Minimal config for local testing

# Model arguments (use a smaller model for testing)
model_name_or_path: microsoft/DialoGPT-small  # Much smaller model for testing
torch_dtype: float32  # Better for CPU testing
attn_implementation: eager  # No flash attention for testing

# System prompt
system_prompt: "You are a medical expert. Answer with <think>reasoning</think><answer>letter</answer>"

# Dataset Configuration - use a subset
dataset_name: openlifescienceai/medmcqa
dataset_train_split: train
dataset_test_split: validation

# MINIMAL training settings
bf16: false  # Use float32 for CPU compatibility
learning_rate: 1.0e-04
num_train_epochs: 1
max_steps: 3  # Only 3 steps for testing!
do_eval: false
gradient_checkpointing: false

# Generation parameters - minimal
max_prompt_length: 128
max_completion_length: 256
num_generations: 2  # Single generation for speed
gradient_accumulation_steps: 2
per_device_train_batch_size: 1
temperature: 0.7
use_vllm: false  # Use regular generation
use_liger_kernel: false  # Disable optimizations

# Reward functions - minimal set
reward_funcs:
- multiple_choice
- format

# Output
output_dir: test_output_local
overwrite_output_dir: true
save_strategy: "no"  # Don't save checkpoints
logging_steps: 1
seed: 42

# Disable features for testing
push_to_hub: false
report_to: []  # No wandb for testing